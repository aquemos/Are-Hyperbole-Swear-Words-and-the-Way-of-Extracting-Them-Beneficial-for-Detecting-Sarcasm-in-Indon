{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ke-FOAIhBPr60ov-dZg9Equ625JV8XMV","authorship_tag":"ABX9TyOr+4u5NMW7dmwlDYss136C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QzLCh5l_olRl","executionInfo":{"status":"ok","timestamp":1684465839261,"user_tz":-420,"elapsed":1555,"user":{"displayName":"Novitasari Arlim","userId":"07542260611884870182"}}},"outputs":[],"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd"]},{"cell_type":"code","source":["models = [[\"preprocessed\"], \n","              [\"preprocessed\", \"interjection indonesia\"],\n","              [\"preprocessed\", \"interjection spacy\"],\n","              [\"preprocessed\", \"intensifier indonesia\"],\n","              [\"preprocessed\", \"intensifier spacy\"],\n","              [\"preprocessed\", \"capital letter\"],\n","              [\"preprocessed\", \"punctuation\"],\n","              [\"preprocessed\", \"elongated word\"],\n","              ['preprocessed', 'swear word'],\n","              ['preprocessed', 'interjection indonesia + swear word'],\n","              ['preprocessed', 'interjection spacy + swear word'],\n","          \n","              [\"preprocessed\", \"capital letter\", \"interjection indonesia\"],\n","              [\"preprocessed\", \"capital letter\", \"interjection spacy\"],\n","              [\"preprocessed\", \"capital letter\", \"punctuation\"],\n","              [\"preprocessed\", \"capital letter\", \"elongated word\"],\n","              [\"preprocessed\", \"capital letter\", \"intensifier indonesia\"],\n","              [\"preprocessed\", \"capital letter\", \"intensifier spacy\"],\n","              ['preprocessed', 'capital letter', 'swear word'],\n","              ['preprocessed', 'capital letter', 'interjection indonesia + swear word'],\n","              ['preprocessed', 'capital letter', 'interjection spacy + swear word'],\n","          \n","              [\"preprocessed\", \"elongated word\", \"intensifier indonesia\"],\n","              [\"preprocessed\", \"elongated word\", \"intensifier spacy\"],\n","              [\"preprocessed\", \"elongated word\", \"punctuation\"],\n","              [\"preprocessed\", \"elongated word\", \"interjection indonesia\"],\n","              [\"preprocessed\", \"elongated word\", \"interjection spacy\"],\n","              ['preprocessed', 'elongated word', 'swear word'],\n","              ['preprocessed', 'elongated word', 'interjection indonesia + swear word'],\n","              ['preprocessed', 'elongated word', 'interjection spacy + swear word'],\n","\n","              [\"preprocessed\", \"punctuation\", \"intensifier indonesia\"],\n","              [\"preprocessed\", \"punctuation\", \"intensifier spacy\"],\n","              [\"preprocessed\", \"punctuation\", \"interjection indonesia\"],\n","              [\"preprocessed\", \"punctuation\", \"interjection spacy\"],\n","              ['preprocessed', 'punctuation', 'swear word'],\n","              ['preprocessed', 'punctuation', 'interjection indonesia + swear word'],\n","              ['preprocessed', 'punctuation', 'interjection spacy + swear word'],\n","          \n","\n","              [\"preprocessed\", \"interjection indonesia\", \"intensifier indonesia\", \"capital letter\", \"punctuation\", \"elongated word\"],\n","              [\"preprocessed\", \"interjection spacy\", \"intensifier spacy\", \"capital letter\", \"punctuation\", \"elongated word\"],\n","              ['preprocessed', 'interjection indonesia', 'intensifier indonesia', 'capital letter', 'punctuation', 'elongated word', 'swear word'],\n","              ['preprocessed', 'interjection spacy', 'intensifier spacy', 'capital letter', 'punctuation', 'elongated word', 'swear word'],\n","              ['preprocessed', 'intensifier indonesia', 'capital letter', 'punctuation', 'elongated word', 'interjection indonesia + swear word'],\n","              ['preprocessed', 'intensifier spacy', 'capital letter', 'punctuation', 'elongated word', 'interjection spacy + swear word']\n","              ]"],"metadata":{"id":"OmxNvm4ypIQL","executionInfo":{"status":"ok","timestamp":1684465839262,"user_tz":-420,"elapsed":14,"user":{"displayName":"Novitasari Arlim","userId":"07542260611884870182"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#Train and predict models\n","\n","The example below used SVM classifiers. Specifically, all classifiers in this work utilize Scikit Learn. Note, we use default parameters value in each classifier.\n","\n","\n","\n"],"metadata":{"id":"jynuIsZl8whK"}},{"cell_type":"code","source":["from sklearn.svm import SVC #import classifiers here"],"metadata":{"id":"Cm5cqrjTpIGw","executionInfo":{"status":"ok","timestamp":1684465839262,"user_tz":-420,"elapsed":11,"user":{"displayName":"Novitasari Arlim","userId":"07542260611884870182"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def train_predict(X_train, X_test, y_train, y_test):\n","  #since the features is not only text, we use ColumnTransformers to transform tweets and the combined models. \n","  transformer = ColumnTransformer([('vectorizer', TfidfVectorizer(), 'preprocessed')], remainder='passthrough') #Note, TF-IDF is used to tranform tweets in ColumnTransformers\n","  X_vec_train = transformer.fit_transform(X_train)\n","  X_vec_test = transformer.transform(X_test)\n","  model = SVC(kernel='linear') #change the classifier here\n","  model.fit(X_vec_train, y_train)\n","  y_predict = model.predict(X_vec_test)\n","  return y_predict"],"metadata":{"id":"QcJ6PFzIpICg","executionInfo":{"status":"ok","timestamp":1684465839263,"user_tz":-420,"elapsed":10,"user":{"displayName":"Novitasari Arlim","userId":"07542260611884870182"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["train_dataset = pd.read_excel(train_dataset_path)\n","test_dataset = pd.read_excel(test_dataset_path)"],"metadata":{"id":"Iu4VLzr-9Qki","executionInfo":{"status":"ok","timestamp":1684465841572,"user_tz":-420,"elapsed":2317,"user":{"displayName":"Novitasari Arlim","userId":"07542260611884870182"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["for model in models:\n","  X_train, y_train = train_dataset[model], train_dataset[\"Class\"]\n","  X_test, y_test = test_dataset[model], test_dataset[\"Class\"]\n","\n","  value = train_predict(X_train, X_test, y_train, y_test)\n","  test_dataset['y_predict'] = value"],"metadata":{"id":"EARlI5lc95mV","executionInfo":{"status":"ok","timestamp":1684465842912,"user_tz":-420,"elapsed":1348,"user":{"displayName":"Novitasari Arlim","userId":"07542260611884870182"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["test_dataset.to_excel(path_to_save_predict, index=False)"],"metadata":{"id":"9pDsqe_MDGvu"},"execution_count":null,"outputs":[]}]}